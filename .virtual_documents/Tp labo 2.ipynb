import pandas as pd
import seaborn as sns
import seaborn.objects as so
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.model_selection import GridSearchCV
from sklearn.neighbors import KNeighborsClassifier, NearestNeighbors
from sklearn.model_selection import LeaveOneOut
from sklearn.cluster import KMeans, DBSCAN
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.decomposition import PCA
from sklearn import linear_model 
from sklearn.metrics import mean_squared_error, r2_score 

import logging
import os

"""
# Estas dos lineas son para silenciar las advertencias de TensorFlow
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'
logging.getLogger('tensorflow').setLevel(logging.ERROR)
"""

import tensorflow as tf
import keras
#from tf_regressor import train_test_split_scale_center


data_raw = pd.read_csv("FBRef2020-21.csv")


data_proc = (data_raw.query("Min >= 500")
             .drop(columns = [column for column in data_raw.columns if data_raw[column].isna().sum() > 100]))

display(data_proc[data_proc.isna().sum(axis=1) != 0])

display(data_proc.loc[313].isna().sum())
display(data_proc.loc[1190].isna().sum())
display(data_proc.loc[2525].isna().sum())
display(data_proc.loc[2525].isna().sort_values())


data_proc = data_proc.drop(index=[313,1190,2525]).reset_index()


data_num = data_proc.drop(columns = [col for col in data_proc.columns if data_proc[col].dtype not in ['float64','int64']])
data_num = data_num.drop(columns = data_num.columns.tolist()[:data_num.columns.tolist().index('Ast/90')])










data_procesada = data_num.drop(columns=[col for col in data_num.columns if col not in ["Miscontrol/90","SoTs/90"]])
(
    so.Plot(data= data_procesada, x= "Miscontrol/90", y= "SoTs/90")
    .add(so.Dot())
)








X_star = MinMaxScaler((0,1)).set_output(transform="pandas").fit_transform(data_num)

X_star_t = np.transpose(X_star)
N = len(X_star)
Sigma = (X_star_t @ X_star) / N



gamma, U = np.linalg.eigh(Sigma)

gamma_flip = np.flip(gamma)
U = np.flip(U, 1)

Z = X_star @ U 
Z.columns = ["Z" + str(i) for i in range(1,104)]


(
    so.Plot(data = Z, x = "Z1", y = "Z2", color = data_proc['Pos'].apply(lambda x: x.split(',')[0]))
    .add(so.Dot())
    .layout(size=(12,8))    
)








kmeans = KMeans (n_clusters = 2)

etiquetas = kmeans.fit_predict(data_num).astype(str)

(
    so.Plot()
    .add(so.Dot(), data = Z, x = "Z1", y = "Z2", color = etiquetas)
    .layout(size=(12,8))
)








clustering = DBSCAN (eps= 8,min_samples = 2)

etiquetas = clustering.fit_predict(Z)

(
    so.Plot(data = Z, x = "Z1", y = "Z2", color = etiquetas.astype(str))
    .add(so.Dot())
    .layout(size=(12,8))
)






outliers = 0

for valor in etiquetas:
    if valor == -1:
        outliers += 1

outliers

clustering2 = DBSCAN (eps= 0.1,min_samples = 23)

etiquetas = clustering2.fit_predict(data_procesada)
(
    so.Plot(data = data_procesada, x= "Miscontrol/90", y= "SoTs/90", color = etiquetas.astype(str))
    .add(so.Dot())
    .layout(size=(12,8))
)




neighbors = NearestNeighbors(n_neighbors=2)  # Esta función nos devuelve los más cercanos incluyendo a si mismo, por eso tomamos 2.
neighbors_fit = neighbors.fit(X_star)
distances, indices = neighbors_fit.kneighbors(X_star)
distances = distances[:,1]
distances = np.sort(distances, axis=0)

so.Plot(x = np.arange(len(distances)), y = distances).add(so.Line())


clustering2 = DBSCAN(eps= 0.85,min_samples = 20)

etiquetas = clustering2.fit_predict(X_star)

(
    so.Plot(data = Z, x= "Z1", y= "Z2", color = etiquetas.astype(str))
    .add(so.Dot())
    .layout(size=(12,8))
)


outliers = 0

for valor in etiquetas:
    if valor == -1:
        outliers += 1

outliers








data_clasif = data_proc.drop(columns = [col for col in data_proc.columns if data_proc[col].dtype not in ['float64','int64']])
data_clasif = data_clasif.drop(columns = data_num.columns.tolist()[:data_num.columns.tolist().index('Ast/90')]).join(data_proc['Pos'].apply(lambda x: x.split(',')[0])).rename(columns = {'Pos': 'Pos_filt'})

X_train, X_test, y_train, y_test = train_test_split(data_num, data_clasif['Pos_filt'], test_size=0.2, random_state=42)

clsf = GridSearchCV(estimator = KNeighborsClassifier(),param_grid={'n_neighbors':[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]}, cv = LeaveOneOut(), verbose=1)
clsf.fit(X_train,y_train)


model = KNeighborsClassifier(n_neighbors = 8)
model.fit(X_train,y_train)
score = model.score(X_test, y_test)
print(score)






data_comp_princ = Z.drop(columns= Z.columns.tolist()[2:])

X_train, X_test, y_train, y_test = train_test_split(data_comp_princ, data_clasif['Pos_filt'], test_size=0.2, random_state=42)

clsf = GridSearchCV(estimator = KNeighborsClassifier(),param_grid={'n_neighbors':[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]}, cv = LeaveOneOut(), verbose=1)
clsf.fit(X_train,y_train)

model = KNeighborsClassifier(n_neighbors = clsf.best_params_['n_neighbors'])
model.fit(X_train,y_train)
score = model.score(X_test, y_test)
print(score)





data_liga_f = pd.read_csv("superleague2023.csv")
data_KNN_liga_f = data_liga_f.drop(columns = data_liga_f.columns.tolist()[:data_liga_f.columns.tolist().index('MP')]).join(data_liga_f['Pos'].apply(lambda x: x.split(',')[0])).rename(columns = {'Pos': 'Pos_filt'})
display(data_KNN_liga_f.isna().sum()) #No hace falta filtrar ningun dato vacío
display(data_KNN_liga_f)



data_KNN_liga_f['Min'] = data_KNN_liga_f['Min'].str.replace(',','').apply(int)

X_train, X_test, y_train, y_test = train_test_split(data_KNN_liga_f.drop(columns='Pos_filt'), data_KNN_liga_f['Pos_filt'], test_size=0.2, random_state=69)

model = KNeighborsClassifier(n_neighbors = clsf.best_params_['n_neighbors'])
model.fit(X_train,y_train)
score = model.score(X_test, y_test)
print(score)





X_train, X_test, y_train, y_test = train_test_split(data_KNN_liga_f.drop(columns='Pos_filt'), data_KNN_liga_f['Pos_filt'], test_size=0.2, random_state=69)

clsf = GridSearchCV(estimator = KNeighborsClassifier(),param_grid={'n_neighbors':[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]}, cv = LeaveOneOut(), verbose=1)
clsf.fit(X_train,y_train)

model = KNeighborsClassifier(n_neighbors = clsf.best_params_['n_neighbors'])
model.fit(X_train,y_train)
score = model.score(X_test, y_test)
print(score)





pca_liga_f = PCA(n_components=5)
Z_liga_f = pca_liga_f.fit_transform(data_KNN_liga_f.drop(columns='Pos_filt'))

X_train, X_test, y_train, y_test = train_test_split(Z_liga_f, data_KNN_liga_f['Pos_filt'], test_size=0.2, random_state=808017424)

clsf = GridSearchCV(estimator = KNeighborsClassifier(),param_grid={'n_neighbors':[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]}, cv = LeaveOneOut(), verbose=1)
clsf.fit(X_train,y_train)

model = KNeighborsClassifier(n_neighbors = clsf.best_params_['n_neighbors'])
model.fit(X_train,y_train)
score = model.score(X_test, y_test)

print(score)





liga_f_500 = data_KNN_liga_f[data_KNN_liga_f['Min'] >= 500]

X_train, X_test, y_train, y_test = train_test_split(liga_f_500.drop(columns='Pos_filt'), liga_f_500['Pos_filt'], test_size=0.2, random_state=808017424)

clsf = GridSearchCV(estimator = KNeighborsClassifier(),param_grid={'n_neighbors':[2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20]}, cv = LeaveOneOut(), verbose=1)
clsf.fit(X_train,y_train)

model = KNeighborsClassifier(n_neighbors = clsf.best_params_['n_neighbors'])
model.fit(X_train,y_train)
score = model.score(X_test, y_test)

print(score)








data = pd.read_csv ("transfermarkt_fbref_201920.csv", delimiter =";")

data_num_players = data.drop(columns = [col for col in data.columns if (data[col].dtype not in ['float64','int64']) & (col != "player")])
display(data_num_players.head())


data = data_num_players.dropna()
data = data.query("minutes >= 500")


X, y = data.drop(columns = [col for col in data.columns if (col == "player") | 
                                        (col == "value") | (col == "birth_year") | (col == "Column1") ]), data["value"]
alphas = np.arange(0,20,0.1)

Param_Grid = {"alpha": alphas}
h = GridSearchCV(linear_model.Ridge(fit_intercept = True),param_grid= Param_Grid,cv=5,scoring="r2",return_train_score= True,verbose= True,error_score='raise')

scal = MinMaxScaler((0,1))
X_scal = scal.fit_transform(X)
y_scal = scal.fit_transform(pd.DataFrame(y))

X_train, X_test, y_train, y_test = train_test_split(X_scal, y, test_size=0.2, random_state=23)

h.fit(X_train,y_train)

modeloRidge = linear_model.Ridge(alpha = 4.2, fit_intercept = True)

modeloRidge.fit(X_train, y_train)

y_pred = modeloRidge.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))


#print(f'Mejor r2: {h.best_score_}')
print(f'Mejor alpha: {h.best_estimator_}')
print(f'r2: {modeloRidge.score(X_test, y_test)}')
print(f'rmse: {rmse}')



